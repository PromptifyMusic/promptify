{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YHq3Hipyp4Ye"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBsffPyhv4A9",
        "outputId": "04ca797e-2b54-4b0b-a673-7ffccb9aaea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/projekt_inzynierski\n"
          ]
        }
      ],
      "source": [
        "%cd /\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "!ln -s /content/gdrive/My\\ Drive/ ./mydrive\n",
        "\n",
        "%cd /content/gdrive/MyDrive/projekt_inzynierski"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start"
      ],
      "metadata": {
        "id": "BR2M8Vwfx6_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "pWsIaa0FpEJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu sentence-transformers\n",
        "!pip install sentence-transformers -q\n",
        "!pip install gliner\n",
        "!pip install spacy gliner transformers torch\n",
        "!python -m spacy download pl_core_news_lg\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download pl_core_news_sm\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "L-rBZIfL-0lJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_18slSkWy-GA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import faiss\n",
        "import spacy\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gliner import GLiNER\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.util import filter_spans\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Modele\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_e5 = SentenceTransformer('intfloat/multilingual-e5-base', device=device)\n",
        "model_gliner = GLiNER.from_pretrained(\"urchade/gliner_multi-v2.1\")\n",
        "nlp_pl = spacy.load(\"pl_core_news_lg\")\n",
        "nlp_en = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Dane\n",
        "df_songs = pd.read_parquet(\"./df_full_with_embeddings.parquet\")\n",
        "df_tag_embeddings = pd.read_parquet(\"./df_unique_tag_embeddings.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_songs.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "CFA31ehJwqD1",
        "outputId": "61948a01-d025-423c-9ebb-56570bfcf714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             track_id            name       artist  \\\n",
              "0  TRIOREW128F424EAF0  Mr. Brightside  The Killers   \n",
              "1  TRRIVDJ128F429B0E8      Wonderwall        Oasis   \n",
              "\n",
              "                                 spotify_preview_url  \\\n",
              "0  https://p.scdn.co/mp3-preview/4d26180e6961fd46...   \n",
              "1  https://p.scdn.co/mp3-preview/d012e536916c927b...   \n",
              "\n",
              "                                                tags genre  year  duration_ms  \\\n",
              "0  rock, alternative, indie, alternative_rock, in...  None  2004       222200   \n",
              "1  rock, alternative, indie, pop, alternative_roc...  None  2006       258613   \n",
              "\n",
              "   danceability  energy  ...  popularity  \\\n",
              "0         0.355   0.918  ...        88.0   \n",
              "1         0.409   0.892  ...        80.0   \n",
              "\n",
              "                                         spotify_url  explicit  \\\n",
              "0  https://open.spotify.com/track/003vvx7Niy0yvhv...     False   \n",
              "1  https://open.spotify.com/track/5qqabIl2vWzo9Ap...     False   \n",
              "\n",
              "                                        album_images              spotify_id  \\\n",
              "0  [{\"height\": 640, \"width\": 640, \"url\": \"https:/...  003vvx7Niy0yvhvHt4a68B   \n",
              "1  [{\"height\": 640, \"width\": 640, \"url\": \"https:/...  5qqabIl2vWzo9ApSC317sa   \n",
              "\n",
              "    n_tempo  n_loudness                                          tags_list  \\\n",
              "0  0.619996    0.874265  ['rock', 'alternative', 'indie', 'alternative_...   \n",
              "1  0.730137    0.874061  ['rock', 'alternative', 'indie', 'pop', 'alter...   \n",
              "\n",
              "   tags_count                                     tags_embedding  \n",
              "0           6  [-0.005010171327739954, 0.01898571476340294, -...  \n",
              "1           9  [0.003282089252024889, 0.02081618271768093, -0...  \n",
              "\n",
              "[2 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab3684e9-5b17-4171-a40d-b0d02707446f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>track_id</th>\n",
              "      <th>name</th>\n",
              "      <th>artist</th>\n",
              "      <th>spotify_preview_url</th>\n",
              "      <th>tags</th>\n",
              "      <th>genre</th>\n",
              "      <th>year</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>...</th>\n",
              "      <th>popularity</th>\n",
              "      <th>spotify_url</th>\n",
              "      <th>explicit</th>\n",
              "      <th>album_images</th>\n",
              "      <th>spotify_id</th>\n",
              "      <th>n_tempo</th>\n",
              "      <th>n_loudness</th>\n",
              "      <th>tags_list</th>\n",
              "      <th>tags_count</th>\n",
              "      <th>tags_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRIOREW128F424EAF0</td>\n",
              "      <td>Mr. Brightside</td>\n",
              "      <td>The Killers</td>\n",
              "      <td>https://p.scdn.co/mp3-preview/4d26180e6961fd46...</td>\n",
              "      <td>rock, alternative, indie, alternative_rock, in...</td>\n",
              "      <td>None</td>\n",
              "      <td>2004</td>\n",
              "      <td>222200</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.918</td>\n",
              "      <td>...</td>\n",
              "      <td>88.0</td>\n",
              "      <td>https://open.spotify.com/track/003vvx7Niy0yvhv...</td>\n",
              "      <td>False</td>\n",
              "      <td>[{\"height\": 640, \"width\": 640, \"url\": \"https:/...</td>\n",
              "      <td>003vvx7Niy0yvhvHt4a68B</td>\n",
              "      <td>0.619996</td>\n",
              "      <td>0.874265</td>\n",
              "      <td>['rock', 'alternative', 'indie', 'alternative_...</td>\n",
              "      <td>6</td>\n",
              "      <td>[-0.005010171327739954, 0.01898571476340294, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRRIVDJ128F429B0E8</td>\n",
              "      <td>Wonderwall</td>\n",
              "      <td>Oasis</td>\n",
              "      <td>https://p.scdn.co/mp3-preview/d012e536916c927b...</td>\n",
              "      <td>rock, alternative, indie, pop, alternative_roc...</td>\n",
              "      <td>None</td>\n",
              "      <td>2006</td>\n",
              "      <td>258613</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.892</td>\n",
              "      <td>...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>https://open.spotify.com/track/5qqabIl2vWzo9Ap...</td>\n",
              "      <td>False</td>\n",
              "      <td>[{\"height\": 640, \"width\": 640, \"url\": \"https:/...</td>\n",
              "      <td>5qqabIl2vWzo9ApSC317sa</td>\n",
              "      <td>0.730137</td>\n",
              "      <td>0.874061</td>\n",
              "      <td>['rock', 'alternative', 'indie', 'pop', 'alter...</td>\n",
              "      <td>9</td>\n",
              "      <td>[0.003282089252024889, 0.02081618271768093, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab3684e9-5b17-4171-a40d-b0d02707446f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab3684e9-5b17-4171-a40d-b0d02707446f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab3684e9-5b17-4171-a40d-b0d02707446f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-16705dfc-523e-4eac-b9ef-94a517486dbf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16705dfc-523e-4eac-b9ef-94a517486dbf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-16705dfc-523e-4eac-b9ef-94a517486dbf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_songs"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konfiguracja"
      ],
      "metadata": {
        "id": "epY5zSxApnY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    # Ekstrakcja fraz\n",
        "    \"gliner_threshold\": 0.3,\n",
        "    \"tag_similarity_threshold\": 0.65,\n",
        "\n",
        "    # Scoring\n",
        "    \"use_idf\": True,\n",
        "    \"query_pow\": 1.0,\n",
        "    \"audio_weight\": 0.4,\n",
        "\n",
        "    # Retrieval\n",
        "    \"n_candidates\": 400,\n",
        "    \"flat_delta\": 0.05,\n",
        "\n",
        "    # Tiery jakościowe\n",
        "    \"min_absolute_high\": 0.75,\n",
        "    \"min_absolute_mid\": 0.50,\n",
        "    \"target_pool_size\": 100,\n",
        "    \"min_required_size\": 15,\n",
        "    \"popularity_rescue_ratio\": 0.2,\n",
        "\n",
        "    # Popularność\n",
        "    \"p_high\": 70,\n",
        "    \"p_mid\": 35,\n",
        "    \"mix\": {\"high\": 0.40, \"mid\": 0.35, \"low\": 0.25},\n",
        "    \"forced_popular\": 2,\n",
        "    \"forced_popular_min\": 80,\n",
        "\n",
        "    # Sampling\n",
        "    \"final_n\": 15,\n",
        "    \"alpha\": 2.0,\n",
        "    \"shuffle\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "GGBEViF29lUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie danych"
      ],
      "metadata": {
        "id": "N9D_vZ6jptYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozszerzenie tagów o kategorie nadrzędne\n",
        "PARENT_RULES = {\n",
        "    \"progressive rock\": \"rock\", \"indie rock\": \"rock\", \"hard rock\": \"rock\",\n",
        "    \"heavy metal\": \"metal\", \"death metal\": \"metal\", \"black metal\": \"metal\",\n",
        "    \"indie pop\": \"pop\", \"synthpop\": \"pop\",\n",
        "    \"drum and bass\": \"electronic\",\n",
        "}\n",
        "\n",
        "KEYWORD_PARENTS = {\n",
        "    \"rock\": [\"rock\"], \"metal\": [\"metal\"], \"pop\": [\"pop\", \"britpop\"],\n",
        "    \"hip hop\": [\"hip hop\", \"rap\"], \"electronic\": [\"electronic\", \"techno\", \"house\"],\n",
        "    \"jazz\": [\"jazz\"], \"punk\": [\"punk\"], \"folk\": [\"folk\"],\n",
        "}\n",
        "\n",
        "def expand_tags(tag_list):\n",
        "    if not tag_list or (isinstance(tag_list, float) and pd.isna(tag_list)):\n",
        "        return []\n",
        "    tags = {str(t).lower().replace(\"_\", \" \").strip() for t in tag_list}\n",
        "\n",
        "    for child, parent in PARENT_RULES.items():\n",
        "        if child in tags:\n",
        "            tags.add(parent)\n",
        "    for parent, kws in KEYWORD_PARENTS.items():\n",
        "        if any(kw in t for t in tags for kw in kws):\n",
        "            tags.add(parent)\n",
        "    return sorted(tags)\n",
        "\n",
        "def _ensure_list(x):\n",
        "    if isinstance(x, list): return x\n",
        "    if pd.isna(x) or x is None: return []\n",
        "    return [t.strip() for t in str(x).split(\",\") if t.strip()]\n",
        "\n",
        "df_songs[\"tags_list\"] = df_songs[\"tags\"].apply(_ensure_list).apply(expand_tags)\n",
        "df_songs[\"tag_count\"] = df_songs[\"tags_list\"].apply(len)\n",
        "df_songs = df_songs.reset_index(drop=True)\n",
        "\n",
        "# Inverted Index\n",
        "def build_inverted_index(df, tags_col=\"tags_list\"):\n",
        "    inv = defaultdict(list)\n",
        "    for i, tags in enumerate(df[tags_col]):\n",
        "        for t in (tags or []):\n",
        "            inv[t].append(i)\n",
        "    return {t: np.array(v, dtype=np.int32) for t, v in inv.items()}\n",
        "\n",
        "INV_INDEX = build_inverted_index(df_songs)\n",
        "N_SONGS = len(df_songs)\n",
        "\n",
        "# Embeddingi tagów\n",
        "TAG_VECS = np.array(df_tag_embeddings[\"tag_embedding\"].tolist(), dtype=np.float32)\n",
        "TAGS = df_tag_embeddings[\"tag\"].tolist()"
      ],
      "metadata": {
        "id": "dzJsUnv_9n1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eksperymenty"
      ],
      "metadata": {
        "id": "X4vjruaRxyad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksperyment 1: Bezpośrednie Wyszukiwanie Wektorowe (Naive Embedding Search)"
      ],
      "metadata": {
        "id": "zVUyVIqPw0Xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opis podejścia: W tym eksperymencie wektor promptu użytkownika był porównywany bezpośrednio z wektorem reprezentującym tagi utworu (kolumna tags_embedding w bazie danych). Wektor utworu powstał poprzez uśrednienie wektorów wszystkich jego tagów. Do wyszukiwania użyto indeksu FAISS (IndexFlatIP).\n",
        "\n",
        "Dlaczego to nie zadziałało? (Wnioski):\n",
        "1. Problem \"Rozmycia\" (Dilution Issue): Piosenki posiadające dużo tagów (np. hity mające tagi: \"rock, pop, alternative, 90s, vocal, classic\") miały wektor będący średnią tych wszystkich pojęć. Tracił on wyrazistość w konkretnych kierunkach.\n",
        "\n",
        "2. Niska precyzja dla krótkich zapytań: Przy zapytaniu \"rock\", algorytm faworyzował niszowe utwory opisane wyłącznie tagiem \"rock\". Utwory popularne (z wieloma tagami) miały niższy wynik podobieństwa, mimo że były poprawne gatunkowo.\n",
        "\n",
        "Decyzja: Metodę zastąpiono podejściem Inverted Index + Scoring, które ocenia każdy tag niezależnie."
      ],
      "metadata": {
        "id": "5vfnkB94zuv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_naive_search(test_prompt):\n",
        "    print(\"--- Uruchamianie Eksperymentu 1: Naive Search ---\")\n",
        "\n",
        "    if \"tags_embedding\" not in df_songs.columns:\n",
        "        print(\"Błąd: Brak kolumny 'tags_embedding'. Eksperyment niemożliwy.\")\n",
        "        return None\n",
        "\n",
        "    embeddings = np.array(df_songs[\"tags_embedding\"].to_list()).astype(\"float32\")\n",
        "\n",
        "    d = embeddings.shape[1]\n",
        "    index_naive = faiss.IndexFlatIP(d)\n",
        "    index_naive.add(embeddings)\n",
        "\n",
        "    print(f\"Zbudowano indeks FAISS dla {index_naive.ntotal} utworów.\")\n",
        "\n",
        "    def search_songs_naive(prompt: str, top_k: int = 500, threshold: float = None):\n",
        "        q_emb = model_e5.encode(\n",
        "            [f\"query: {prompt}\"],\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True\n",
        "        ).astype(\"float32\")\n",
        "\n",
        "        distances, indices = index_naive.search(q_emb, top_k)\n",
        "        distances = distances[0]\n",
        "        indices = indices[0]\n",
        "\n",
        "        results = df_songs.iloc[indices].copy()\n",
        "        results[\"score_naive\"] = distances\n",
        "\n",
        "        if threshold is not None:\n",
        "            results = results[results[\"score_naive\"] >= threshold]\n",
        "\n",
        "        return results.sort_values(\"score_naive\", ascending=False)\n",
        "\n",
        "    print(f\"\\nTestowanie promptu: '{test_prompt}'\")\n",
        "\n",
        "    results = search_songs_naive(test_prompt, top_k=50, threshold=0.85)\n",
        "\n",
        "    if not results.empty:\n",
        "        display(results[['name', 'artist', 'tags_list', 'popularity', 'score_naive']].head(10))\n",
        "    else:\n",
        "        print(\"Brak wyników spełniających kryteria.\")\n",
        "\n",
        "run_experiment_naive_search(\"rock alternative grunge 90s\")\n",
        "run_experiment_naive_search(\"rock\")"
      ],
      "metadata": {
        "id": "zGLwH98DyUPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksperyment 2: Ekstrakcja cech przez LLM"
      ],
      "metadata": {
        "id": "vg7FFp_YFPyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tym podejściu próbowano wykorzystać generatywną sztuczną inteligencję (Large Language Model), aby \"zrozumiała\" intencję użytkownika i przetłumaczyła ją na ustrukturyzowane wartości cech audio (JSON).\n",
        "\n",
        "Zastosowana metoda:\n",
        "\n",
        "1. Użycie modelu Qwen2.5-1.5B-Instruct (relatywnie lekki model LLM).\n",
        "\n",
        "2. Zastosowanie techniki Prompt Engineering: zdefiniowanie System Prompt, który instruuje model, jak mapować przymiotniki na wartości numeryczne Spotify (0.0 - 1.0) i wymusza format wyjściowy JSON.\n",
        "\n",
        "Dlaczego to nie zadziałało? (Wnioski):\n",
        "\n",
        "1. Niska jakość odwzorowania (Gubienie informacji): Model wykazuje tendencję do pomijania istotnych słów kluczowych.\n",
        "\n",
        "    * Przykład błędu: Dla zapytania \"bardzo szybka i wesoła muzyka do tańca\", model zwrócił JSON: {\"n_tempo\": 0.9, \"danceability\": 0.9}, całkowicie ignorując słowo \"wesoła\" (brak parametru valence).\n",
        "\n",
        "    * Pokazuje to, że mimo dużej mocy obliczeniowej, LLM bez zaawansowanego fine-tuningu jest mniej przewidywalny niż systemy regułowe.\n",
        "\n",
        "2. Zbyt duży narzut czasowy (Latency): Generowanie odpowiedzi przez LLM trwało od kilku do kilkunastu sekund. W systemie wyszukiwania, który ma działać w czasie rzeczywistym, jest to nieakceptowalne.\n",
        "\n",
        "3. Wymagania sprzętowe: Nawet mały model (1.5B parametrów) obciąża pamięć GPU (VRAM), co utrudnia wdrażanie systemu.\n",
        "\n",
        "Decyzja: Zastąpiono to podejściem hybrydowym (GLiNER + Reguły), które jest tysiące razy szybsze i w pełni deterministyczne."
      ],
      "metadata": {
        "id": "Re208Yb3Dbeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "\n",
        "SYSTEM_PROMPT_LLM = \"\"\"You are a music feature extractor. Extract Spotify audio features from user's description.\n",
        "\n",
        "Features (0.0 to 1.0):\n",
        "- valence: 0.1=very sad, 0.9=very happy\n",
        "- energy: 0.1=calm, 0.9=intense\n",
        "- n_tempo: 0.1=very slow, 0.9=very fast\n",
        "- danceability: 0.1=not danceable, 0.9=very danceable\n",
        "- acousticness: 0.1=electronic, 0.9=acoustic\n",
        "- instrumentalness: 0.1=vocals, 0.9=no vocals\n",
        "\n",
        "Return ONLY features mentioned in description as JSON. No explanation.\n",
        "Example:\n",
        "User: \"szybka energiczna bez słów\"\n",
        "Output: {\"n_tempo\": 0.8, \"energy\": 0.9, \"instrumentalness\": 0.9}\n",
        "\"\"\"\n",
        "\n",
        "def run_experiment_llm(user_prompt):\n",
        "    print(f\"--- Uruchamianie Eksperymentu 2: LLM Extraction dla '{user_prompt}' ---\")\n",
        "\n",
        "    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    print(f\"Ładowanie modelu {model_name} (może to potrwać chwilę)...\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd ładowania modelu LLM (możliwy brak pamięci VRAM): {e}\")\n",
        "        return\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_LLM},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    print(\"Generowanie odpowiedzi...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    print(f\"Surowa odpowiedź modelu:\\n{response}\")\n",
        "\n",
        "    try:\n",
        "        start = response.find('{')\n",
        "        end = response.rfind('}') + 1\n",
        "        if start != -1 and end > start:\n",
        "            json_str = response[start:end]\n",
        "            features = json.loads(json_str)\n",
        "            print(f\"\\nZinterpretowane cechy (JSON):\\n{features}\")\n",
        "            return features\n",
        "        else:\n",
        "            print(\"Nie znaleziono JSON w odpowiedzi.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Błąd parsowania JSON.\")"
      ],
      "metadata": {
        "id": "x1DIKjgU-q6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment_llm(\"bardzo szybka i wesoła muzyka do tańca\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmwL7mHMrKJw",
        "outputId": "a6466625-1dd5-41bf-dbcc-f5bf02217bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Uruchamianie Eksperymentu 2: LLM Extraction dla 'bardzo szybka i wesoła muzyka do tańca' ---\n",
            "Ładowanie modelu Qwen/Qwen2.5-1.5B-Instruct (może to potrwać chwilę)...\n",
            "Generowanie odpowiedzi...\n",
            "Surowa odpowiedź modelu:\n",
            "{\"n_tempo\": 0.9, \"danceability\": 0.9, \"valence\": 0.9}\n",
            "\n",
            "Zinterpretowane cechy (JSON):\n",
            "{'n_tempo': 0.9, 'danceability': 0.9, 'valence': 0.9}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_tempo': 0.9, 'danceability': 0.9, 'valence': 0.9}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rozważano wykorzystanie modelu LLM (np. Llama-3-8B lub Qwen) w roli \"inteligentnego routera\". Jego zadaniem miał być podział promptu na frazy i przypisanie im kategorii (Tag, Cecha Audio, Aktywność) w jednym przebiegu, zwracając gotowy JSON.\n",
        "\n",
        "Uzasadnienie odrzucenia (Decyzja Architektoniczna): Rozwiązanie to uznano za nieoptymalne z następujących powodów:\n",
        "\n",
        "Latency (Opóźnienie): Czekanie ~1000ms na sam parsing zapytania przez LLM drastycznie pogarsza User Experience w systemie real-time.\n",
        "\n",
        "Niedeterministyczność: Testy wykazały (patrz: Eksperyment 2), że LLM potrafi gubić informacje (np. pominął cechę valence przy zapytaniu \"wesoła muzyka\") lub halucynować tagi.\n",
        "\n",
        "Brak precyzji w ekstrakcji granic: LLM \"przepisuje\" tekst, czasem go zmieniając. Systemy oparte na ekstraktorach (span extraction) są bezpieczniejsze, bo pracują na oryginalnym tekście.\n",
        "\n",
        "Wdrożone rozwiązanie alternatywne (Pipeline Hybrydowy): Zamiast jednego \"magicznego\" modelu LLM, zdecydowano się na zbudowanie wielostopniowego potoku przetwarzania, który zapewnia pełną kontrolę nad logiką:\n",
        "\n",
        "Ekstrakcja (Segmentation): Wykorzystanie reguł gramatycznych spaCy Matcher do precyzyjnego wycięcia fraz (rzeczowniki, przymiotniki, związki rządu).\n",
        "\n",
        "Wstępny Routing (Classification): Użycie lekkiego modelu GLiNER głównie do identyfikacji twardych encji (Gatunki/Tagi).\n",
        "\n",
        "Logika Fallback & Tournament:\n",
        "\n",
        "Wszystko, czego GLiNER nie rozpozna jako Tag, trafia do weryfikacji jako Cecha Audio lub Aktywność.\n",
        "\n",
        "W przypadku konfliktu (np. fraza pasuje i do Aktywności, i do Audio), uruchamiany jest mechanizm turniejowy (Tournament) oparty na confidence score z wyszukiwania wektorowego (E5).\n",
        "\n",
        "Wniosek: Złożony system regułowy wspierany lekkim modelem ML (GLiNER + E5) okazał się szybszy i bardziej przewidywalny niż jeden duży model LLM."
      ],
      "metadata": {
        "id": "xj6a0F3xDemZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksperyment 3: Audio Anchors (Kotwice Semantyczne) - Odrzucony"
      ],
      "metadata": {
        "id": "vXzfzZFoqBYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opis podejścia: Zamiast sztywnych reguł if/else, zastosowano podejście oparte na podobieństwie semantycznym. Zdefiniowano pary zdań referencyjnych (\"kotwic\") opisujących skrajne wartości cech audio (np. dla cechy energy: zdanie opisujące ciszę vs. zdanie opisujące hałas).\n",
        "\n",
        "Wektor promptu użytkownika był porównywany z wektorami obu kotwic. Wynikowa wartość cechy była obliczana na podstawie tego, do którego bieguna (0.0 czy 1.0) prompt był bardziej podobny matematycznie.\n",
        "\n",
        "Zastsowano podejście testowe. czy jesteśmy w stanie wiele fraz wyciągnąć z 1 całego zdania. Nie jesteśmy.\n",
        "\n",
        "Występuje tu zjawisko **\"zanieczyszczenia semantycznego\" (Semantic Bleed)**.\n",
        "Dla modelu wektorowego słowo \"taniec\" jest semantycznie bliskie słowom \"impreza\", \"radość\".\n",
        "W rezultacie, obecność słowa \"taniec\" może sztucznie zawyżyć wynik `valence` (sprawić, że piosenka zostanie oceniona jako weselsza niż jest w rzeczywistości), mimo że użytkownik wyraźnie napisał \"smutna\".\n",
        "\n",
        "Wniosek był taki, że musieliśmy podzielić najpierw prompt na wiele mniejszych prostych fraz, żeby wyciągnąć poprawnie emocje. Zastosowaliśmy też potem bardziej zniuansowane podejście najpierw z wartościami (i to co jest najbliżej ma najwyższy score). To jednak też się okazało wadliwe, bo lepiej się sprawdzają przedziały (idealne wartości dla np. \"szybka\").\n",
        "\n",
        "Dlaczego to nie zadziałało? (Wnioski):\n",
        "\n",
        "1. Niejednoznaczność: Model E5 czasami \"łapał\" podobieństwo nie tam, gdzie trzeba. Np. słowo \"Impreza\" (Party) w prompcie ciągnęło w górę nie tylko danceability (co jest poprawne), ale też valence (szczęście), co wykluczało \"smutne piosenki do tańca\".\n",
        "\n",
        "2. Problem \"Środka Skali\": Metoda ta miała tendencję do polaryzacji wyników (albo 0.1, albo 0.9), trudno było uzyskać subtelne wartości pośrednie (np. 0.3) i przewagę by miały zbyt skrajne utwory. Zrezygnowano z takiego podejścia.\n",
        "\n",
        "3. Decyzja: Zastąpiono to podejściem, w którym GLiNER i spacy wycina konkretną frazę (np. \"spokojna\"), a system mapuje ją na precyzyjny zakres liczbowy."
      ],
      "metadata": {
        "id": "ZEHpACI3uhON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "co jeszcze próbowaliśmy?\n",
        "próba wyciągnięcia fraz na wiele sposobów. Np samym glinerem,"
      ],
      "metadata": {
        "id": "CxtckZTb6ftd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "główne problemy:\n",
        "  podzielenie fraz ładnie,\n",
        "  routing - tag, audio feature??"
      ],
      "metadata": {
        "id": "UMFB9umm6uKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_PAIRS_SENTENCES = {\n",
        "    \"valence\": (\n",
        "        \"This is suffering, crying, tragedy, depression, heartbreak, misery, death and pain.\",\n",
        "        \"This is happiness, joy, laughter, celebration, fun, smile, excited and party.\"\n",
        "    ),\n",
        "    \"energy\": (\n",
        "        \"The music is very calm, relaxing, quiet, gentle and sleepy.\",\n",
        "        \"The music is energetic, aggressive, intense, fast and powerful.\"\n",
        "    ),\n",
        "    \"danceability\": (\n",
        "        \"This is music for sleeping, studying, sitting and relaxing.\",\n",
        "        \"This is music for dancing, partying, clubs and moving your body.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def experiment_audio_anchors(prompt, model, audio_pairs, min_diff=0.035):\n",
        "    \"\"\"\n",
        "    Oblicza wartości cech audio na podstawie bliskości semantycznej do zdań-kotwic.\n",
        "    \"\"\"\n",
        "    print(f\"--- Analiza Audio Anchors dla: '{prompt}' ---\")\n",
        "    preferences = {}\n",
        "\n",
        "    prompt_vec = model.encode([f\"query: {prompt} music\"], normalize_embeddings=True)[0]\n",
        "\n",
        "    for feature, (neg_text, pos_text) in audio_pairs.items():\n",
        "        neg_vec = model.encode([f\"query: {neg_text}\"], normalize_embeddings=True)[0]\n",
        "        pos_vec = model.encode([f\"query: {pos_text}\"], normalize_embeddings=True)[0]\n",
        "\n",
        "        sim_neg = np.dot(prompt_vec, neg_vec)\n",
        "        sim_pos = np.dot(prompt_vec, pos_vec)\n",
        "\n",
        "        diff = sim_pos - sim_neg\n",
        "\n",
        "        if abs(diff) < min_diff:\n",
        "            continue\n",
        "\n",
        "        score = 0.5 + (diff * 15.0)\n",
        "        score = max(0.0, min(1.0, score))\n",
        "\n",
        "        preferences[feature] = round(float(score), 2)\n",
        "        print(f\"Cecha: {feature:<15} | Neg: {sim_neg:.3f} | Pos: {sim_pos:.3f} | Diff: {diff:.3f} -> Wynik: {score}\")\n",
        "\n",
        "    return preferences\n",
        "\n",
        "test_prompt_anchors = \"smutna piosenka do tańca\"\n",
        "prefs = experiment_audio_anchors(test_prompt_anchors, model_e5, AUDIO_PAIRS_SENTENCES)\n",
        "print(f\"\\nWywnioskowane parametry: {prefs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwJOdpwGIdWF",
        "outputId": "e805c583-96cd-4e82-f2ea-313a706ef350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Analiza Audio Anchors dla: 'smutna piosenka do tańca' ---\n",
            "Cecha: danceability    | Neg: 0.765 | Pos: 0.814 | Diff: 0.049 -> Wynik: 1.0\n",
            "\n",
            "Wywnioskowane parametry: {'danceability': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksperyment 4: Ręczny podział promptu (Intelligent Split / N-grams)"
      ],
      "metadata": {
        "id": "-_VP61yvH3yD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opis podejścia: Próba stworzenia własnego algorytmu do segmentacji tekstu bez użycia sieci neuronowych typu NER. Algorytm działał w oparciu o N-gramy (okno przesuwne):\n",
        "\n",
        "1. Prompt był dzielony na wszystkie możliwe kombinacje słów (1-słowne, 2-słowne, 3-słowne...).\n",
        "\n",
        "2. Każdy n-gram był zamieniany na wektor i porównywany z bazą cech audio.\n",
        "\n",
        "3. Zastosowano algorytm zachłanny (Greedy Approach): wybierano te fragmenty, które miały najwyższe dopasowanie, a następnie \"wykreślano\" użyte słowa, aby uniknąć nakładek.\n",
        "\n",
        "Dlaczego to nie zadziałało? (Wnioski):\n",
        "\n",
        "1. Złożoność obliczeniowa: Generowanie i embeddowanie wszystkich n-gramów dla długiego promptu jest kosztowne.\n",
        "\n",
        "2. Brak kontekstu gramatycznego: Algorytm \"nie widzi\" zależności. Fraza \"nie jest smutna\" mogła zostać pocięta na \"nie\" (ignorowane) i \"smutna\" (wysokie dopasowanie do valence niskiego), co całkowicie zmieniało sens.\n",
        "\n",
        "3. Brak separacji: Matematyka wektorowa (Cosine Similarity) ma tendencję do faworyzowania dłuższych, bogatszych fraz, które \"zawłaszczają\" znaczenie. Długie zdanie \"zjada\" mniejsze, precyzyjne cechy.\n",
        "\n",
        "spaCy Matcher: Opiera się na składni (gramatyce). Dla matchera \"bardzo szybka\" to ADV + ADJ, a \"wesoła\" to osobne ADJ. Matcher nie obchodzi znaczenie słów, tylko ich rola w zdaniu, dlatego idealnie to tnie"
      ],
      "metadata": {
        "id": "I54Be0DBIBYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_DESCRIPTIONS = {\n",
        "    'valence': [\n",
        "        (0.1, \"very low valence, very sad, melancholic, dark, gloomy emotional mood music\"),\n",
        "        (0.3, \"low valence, bittersweet, thoughtful, introspective, moody emotional mood music\"),\n",
        "        (0.5, \"medium valence, neutral emotional mood, neither clearly happy nor clearly sad music\"),\n",
        "        (0.7, \"high valence, positive, pleasant, warm, cheerful, uplifting emotional mood music\"),\n",
        "        (0.9, \"very high valence, very happy, joyful, exstatic, euphoric, bright, feel-good emotional mood music\")\n",
        "    ],\n",
        "\n",
        "    'energy': [\n",
        "        (0.1, \"very low energy, calm, soft, gentle, ambient-like minimal intensity music\"),\n",
        "        (0.3, \"low energy, relaxing, mellow, laid-back, smooth, low intensity music\"),\n",
        "        (0.5, \"medium energy, balanced intensity, moderate dynamic music\"),\n",
        "        (0.7, \"high energy, energetic, dynamic, lively, strong impact music\"),\n",
        "        (0.9, \"very high energy, intense, aggressive, powerful, explosive music\")\n",
        "    ],\n",
        "\n",
        "    'danceability': [\n",
        "        (0.1, \"very low danceability, not danceable, abstract or experimental, weak or irregular rhythm music\"),\n",
        "        (0.3, \"low danceability, little groove, minimal rhythm, not primarily for dancing music\"),\n",
        "        (0.5, \"medium danceability, some groove, steady rhythm, moderately danceable music\"),\n",
        "        (0.7, \"high danceability, clear beat, strong groove, good for dancing, club-oriented music\"),\n",
        "        (0.9, \"very high danceability, strong groove, infectious rhythm, perfect for dancing, party, club banger music\")\n",
        "    ],\n",
        "\n",
        "    'acousticness': [\n",
        "        (0.1, \"very low acousticness, fully electronic, synthetic, digital, computer-generated sound music\"),\n",
        "        (0.3, \"low acousticness, mostly electronic with some subtle organic or acoustic elements music\"),\n",
        "        (0.5, \"medium acousticness, balanced mix of acoustic and electronic instruments, hybrid sound music\"),\n",
        "        (0.7, \"high acousticness, mostly acoustic, organic, live instruments such as accoustic guitar or piano music\"),\n",
        "        (0.9, \"very high acousticness, fully acoustic, unplugged, natural, organic instruments only music\")\n",
        "    ],\n",
        "\n",
        "    'n_tempo': [\n",
        "        (0.1, \"very slow tempo, very slow pace, dragging rhythm music\"),\n",
        "        (0.3, \"slow tempo, downtempo, slow pace, relaxed rhythm music\"),\n",
        "        (0.5, \"medium tempo, moderate pace, walking pace music\"),\n",
        "        (0.7, \"fast tempo, uptempo, quick pace, energetic rhythm music\"),\n",
        "        (0.9, \"very fast tempo, rapid pace, racing rhythm, frantic speed music\")\n",
        "    ],\n",
        "\n",
        "    'instrumentalness': [\n",
        "        (0.1, \"very low instrumentalness, strong presence of vocals and lyrics, clear singing, vocal-focused track\"),\n",
        "        (0.5, \"medium instrumentalness, mix of vocals and instrumental sections, vocals present but not constant\"),\n",
        "        (0.9, \"very high instrumentalness, fully instrumental track, no vocals, no singing, music without lyrics\")\n",
        "    ],\n",
        "\n",
        "    'n_loudness': [\n",
        "        (0.1, \"very low loudness, very quiet music, soft volume, low volume level, delicate dynamics music\"),\n",
        "        (0.3, \"low loudness, quiet music, gentle volume, reduced volume level, subtle overall loudness music\"),\n",
        "        (0.5, \"medium loudness, normal volume, balanced loudness level, neither quiet nor loud music\"),\n",
        "        (0.7, \"high loudness, loud music, strong volume, high loudness level, impactful sound music\"),\n",
        "        (0.9, \"very high loudness, very loud music, extremely strong volume, very high loudness level music\")\n",
        "    ],\n",
        "\n",
        "    'speechiness': [\n",
        "        (0.1, \"very low speechiness, purely musical track, no spoken words, fully melodic music\"),\n",
        "        (0.3, \"low speechiness, mostly music with occasional spoken words or short background phrases\"),\n",
        "        (0.5, \"medium speechiness, balanced mix of speech and music, frequent spoken segments, rap-like or talky structure\"),\n",
        "    ],\n",
        "\n",
        "    'noise': [\n",
        "        # Nouns (Generic terms)\n",
        "        (None, \"music song track playlist list recording audio sound genre style vibe type kind number piece\"),\n",
        "        # Verbs (Search intent)\n",
        "        (None, \"I am looking for I want I need search find play listen to give me recommend show me\"),\n",
        "        # Adjectives (Empty fillers)\n",
        "        (None, \"good very good nice great best cool amazing awesome some any kind of such a\")\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "KrS28aK9IcLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generowanie wektorów dla AUDIO_INDEX...\")\n",
        "AUDIO_INDEX = {}\n",
        "\n",
        "for feat, descs in FEATURE_DESCRIPTIONS.items():\n",
        "    passages = [f\"passage: {d[1]}\" for d in descs]\n",
        "    embeddings = model_e5.encode(passages, normalize_embeddings=True)\n",
        "    AUDIO_INDEX[feat] = embeddings\n",
        "\n",
        "print(\"Gotowe! AUDIO_INDEX zawiera wektory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2CZKK_dN9k-",
        "outputId": "3f2405ad-8b36-40e6-a394-686c61ad10bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generowanie wektorów dla AUDIO_INDEX...\n",
            "Gotowe! AUDIO_INDEX zawiera wektory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(words, min_n=1, max_n=None):\n",
        "    \"\"\"\n",
        "    Generuje wszystkie możliwe n-gramy (ciągłe fragmenty tekstu).\n",
        "    Np. dla [\"szybki\", \"rock\"] -> \"szybki\", \"rock\", \"szybki rock\"\n",
        "    \"\"\"\n",
        "    if max_n is None:\n",
        "        max_n = len(words)\n",
        "\n",
        "    ngrams = []\n",
        "    for n in range(min_n, min(max_n + 1, len(words) + 1)):\n",
        "        for i in range(len(words) - n + 1):\n",
        "            text = \" \".join(words[i:i+n])\n",
        "            ngrams.append((i, i+n, text))\n",
        "    return ngrams\n",
        "\n",
        "def score_text_to_feature(text, feature_embeddings_dict, model):\n",
        "    \"\"\"Oblicza maksymalne podobieństwo tekstu do którejkolwiek cechy w słowniku.\"\"\"\n",
        "    query_emb = model.encode([f\"query: {text}\"], normalize_embeddings=True)\n",
        "\n",
        "    best_score = -1.0\n",
        "    best_feat = None\n",
        "\n",
        "    for feature_name, emb_matrix in feature_embeddings_dict.items():\n",
        "        sims = cosine_similarity(query_emb, emb_matrix)[0]\n",
        "        max_sim = float(np.max(sims))\n",
        "\n",
        "        if max_sim > best_score:\n",
        "            best_score = max_sim\n",
        "            best_feat = feature_name\n",
        "\n",
        "    return best_score, best_feat\n",
        "\n",
        "def experiment_intelligent_split(prompt, feature_embeddings_dict, model, min_confidence=0.75):\n",
        "    print(f\"--- Analiza N-gramowa dla: '{prompt}' ---\")\n",
        "\n",
        "    words = prompt.lower().split()\n",
        "    ngrams = generate_ngrams(words, min_n=1, max_n=min(6, len(words)))\n",
        "\n",
        "    extracted_parts = []\n",
        "    used_positions = set()\n",
        "\n",
        "    for _ in range(5):\n",
        "        best_iter_score = -1\n",
        "        best_iter_match = None\n",
        "\n",
        "        for start, end, text in ngrams:\n",
        "            positions = set(range(start, end))\n",
        "            if positions & used_positions:\n",
        "                continue\n",
        "\n",
        "            score, feat_name = score_text_to_feature(text, feature_embeddings_dict, model)\n",
        "\n",
        "            if score > best_iter_score:\n",
        "                best_iter_score = score\n",
        "                best_iter_match = (start, end, text, feat_name)\n",
        "\n",
        "        if best_iter_score >= min_confidence and best_iter_match:\n",
        "            start, end, text, feat = best_iter_match\n",
        "            print(f\"  Wykryto: '{text}' -> Cecha: {feat} (Score: {best_iter_score:.3f})\")\n",
        "\n",
        "            extracted_parts.append({\n",
        "                'text': text,\n",
        "                'feature': feat,\n",
        "                'score': best_iter_score\n",
        "            })\n",
        "            used_positions.update(range(start, end))\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    extracted_parts.sort(key=lambda x: words.index(x['text'].split()[0]))\n",
        "    return extracted_parts\n",
        "\n",
        "prompt_test = \"bardzo szybka i wesoła muzyka\"\n",
        "results = experiment_intelligent_split(prompt_test, AUDIO_INDEX, model_e5)\n",
        "print(\"\\nWynik segmentacji:\", [r['text'] for r in results])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrYSp_13HxAt",
        "outputId": "7078bad6-3ccc-4682-9dca-8bb48bd19bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Analiza N-gramowa dla: 'bardzo szybka i wesoła muzyka' ---\n",
            "  Wykryto: 'bardzo szybka i wesoła muzyka' -> Cecha: energy (Score: 0.859)\n",
            "\n",
            "Wynik segmentacji: ['bardzo szybka i wesoła muzyka']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. poczatek - proba najprosztszych rozwiazan całościowych\n",
        "2. podział na frazy i przypisanie ich do audio/tag\n",
        "3. osobno podzial na frazy (llm, matcher + gliner (fail), matcher (sam, ost)), a potem próba przypisania routing (tu porownanie niewiele dawalo bo tagi i features audio dzialaja srednio. w poprzednim kroku zauwazono, ze gliner slabo dzieli zdanie na frazy, ale za to swietnie rozpoznaje tagi, więc postawiono zastosować system typu wydziel tagi, potem zostaw resztę (audio features i potem dojda jeszcze czynnosci to je oddzielamy mechanizmem porownania + odrobina matchera)).\n",
        "4. porownanie normalnie taga do tagow, cechy do cech itp. to dziala git.\n",
        "\n",
        "jeszcze byly eksperymenty czy wystarczy progowanie tagow (jesli ma similarity dla jakiegos taga > niz np 0.9 to tag a jak nie to nie), ale z powodu tego, że ten model ma wysoki ten wskaźnik similarity a niską rozbieżność, to prawie wsyztsko miało bardzo wysoki score i bardzo trudno było tak ustawić, żeby idealnie wpadały niektóre.\n",
        "\n",
        "czy nasz system nie jest nadmiernie skomplikowany?\n",
        "\n",
        "zapytac ai czy wyglada na ai przez przesadna komplikacje kodu i unikanie prostowty typowej dla czlowieka (ai zawsze rozwkleka ten kod niemiłosiernie)"
      ],
      "metadata": {
        "id": "5VYpT2puP0s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksperyment 5: GLiNER jako samodzielny ekstraktor (End-to-End)"
      ],
      "metadata": {
        "id": "snYWKWDfXU-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opis podejścia: Próba wykorzystania modelu GLiNER jako jedynego narzędzia do analizy tekstu. Założono, że model ten będzie w stanie samodzielnie:\n",
        "\n",
        "Wykryć granice fraz (gdzie zaczyna się i kończy opis).\n",
        "\n",
        "Przypisać im odpowiednie kategorie (Gatunek, Nastrój, Tempo).\n",
        "\n",
        "Scenariusz testowy: Uruchomienie modelu na złożonym prompcie z wieloma przymiotnikami i sprawdzenie, czy wyłapie wszystkie istotne słowa kluczowe.\n",
        "\n",
        "Dlaczego to nie wystarczyło? (Wnioski):\n",
        "\n",
        "Niestabilność granic (Span Boundaries): GLiNER jest modelem probabilistycznym. Czasami uznawał frazę \"szybki rock\" za jedną encję (Gatunek), a innym razem rozdzielał \"szybki\" i \"rock\". Brak determinizmu utrudniał dalszą logikę.\n",
        "\n",
        "Pominięcia (Missed Entities): Model świetnie radzi sobie z rzeczownikami (gatunki), ale gorzej z luźnymi przymiotnikami (np. \"fajna\", \"dobra\", \"mocna\"), jeśli nie pasują idealnie do definicji etykiety.\n",
        "\n",
        "Decyzja (Podejście Hybrydowe):\n",
        "\n",
        "Ekstrakcja (Segmentation): Przeniesiono do spaCy Matcher. Reguły gramatyczne (np. PRZYMIOTNIK + RZECZOWNIK) są sztywne i zawsze wytną poprawny fragment tekstu.\n",
        "\n",
        "Klasyfikacja (Routing): GLiNER został zdegradowany do roli \"klasyfikatora\" (Routera) – dostaje już wycięty kawałek i decyduje tylko, czy to Tag (Gatunek), czy Audio Feature."
      ],
      "metadata": {
        "id": "aS_aVKb7Xc6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_gliner_standalone(prompt):\n",
        "    print(f\"--- GLiNER Standalone dla: '{prompt}' ---\")\n",
        "\n",
        "    labels = [\n",
        "        \"gatunek muzyczny\",\n",
        "        \"nastrój\",\n",
        "        \"tempo\",\n",
        "        \"przeznaczenie (aktywność)\"\n",
        "    ]\n",
        "\n",
        "    entities = model_gliner.predict_entities(prompt, labels, threshold=0.1)\n",
        "\n",
        "    print(f\"Znalezione encje:\")\n",
        "    found_texts = []\n",
        "    for e in entities:\n",
        "        print(f\"  • '{e['text']}' -> {e['label']} ({e['score']:.2f})\")\n",
        "        found_texts.append(e['text'])\n",
        "\n",
        "    return found_texts\n",
        "\n",
        "prompt_gliner = \"szukam bardzo szybkiej muzyki rockowej do biegania i czegoś wolnego\"\n",
        "\n",
        "print(\"Oczekiwanie: Wykrycie 'bardzo szybkiej', 'rockowej', 'do biegania', 'wolnego'\")\n",
        "detected = experiment_gliner_standalone(prompt_gliner)\n",
        "\n",
        "expected_keywords = [\"szybkiej\", \"rockowej\", \"biegania\", \"wolnego\"]\n",
        "missing = [word for word in expected_keywords if not any(word in d for d in detected)]\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\nPorażka: Model pominął słowa: {missing}\")\n",
        "    print(\"Wniosek: GLiNER gubi przymiotniki lub łączy je w zbyt długie bloki.\")\n",
        "else:\n",
        "    print(\"\\nUwaga: Nawet jeśli wykrył wszystko, analiza granic (gdzie kończy się opis) jest niepewna.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVJwzqFfXWCK",
        "outputId": "cae6dbb3-368b-49d1-c001-78957f55ff07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oczekiwanie: Wykrycie 'bardzo szybkiej', 'rockowej', 'do biegania', 'wolnego'\n",
            "--- GLiNER Standalone dla: 'szukam bardzo szybkiej muzyki rockowej do biegania i czegoś wolnego' ---\n",
            "Znalezione encje:\n",
            "  • 'szukam' -> przeznaczenie (aktywność) (0.22)\n",
            "  • 'bardzo szybkiej' -> tempo (0.20)\n",
            "  • 'muzyki rockowej' -> gatunek muzyczny (0.80)\n",
            "  • 'biegania' -> przeznaczenie (aktywność) (0.47)\n",
            "  • 'wolnego' -> przeznaczenie (aktywność) (0.41)\n",
            "\n",
            "Uwaga: Nawet jeśli wykrył wszystko, analiza granic (gdzie kończy się opis) jest niepewna.\n"
          ]
        }
      ]
    }
  ]
}